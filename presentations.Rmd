
<br>

<img src="/images/rocks.jpg" width = 1000 />

<br>

#### <font color="#2B547E"> A Shiny App for Identifying Invasive Carp Using Random Forests and Fish Egg Characteristics </font>

**February 25, 2021** - Talk for [ISU Graphics Group](https://isu-graphics.rbind.io/)

The fish species of Grass Carp (*Ctenopharyngodon idella*), Silver Carp (*Hypophthalmichthys molitrix*), and Bighead Carp (*H. nobilis*) are categorized as invasive carp in North America. There is interest from a natural resource management perspective to monitor the populations and spread of the fish species. A common monitoring practice is to collect and genetically identify fish eggs, but this process is both costly in money and time. [Camacho et al. (2019)](https://afspubs.onlinelibrary.wiley.com/doi/abs/10.1002/nafm.10380) demonstrated the use of machine learning as a possibility for a more efficient method of identifying invasive carp. Camacho et al. (2019) trained random forests on easy to measure egg characteristics such as water conductivity and average membrane diameter, and the models returned high accuracy.

In this talk, I will present my recent work with Dr. Michael Weber (NREM) and Dr. Philip Dixon on a Shiny app (WhoseEgg) for identifying invasive carp using random forests based on those from Camacho et al. (2019). The app is intended to be a tool for researchers to input their own fish egg data and easily obtain random forest predictions via a point-and-click user interface. We began work on the app in January, so it is still in development. I will share the current state of the app and our goals for enhancement. I will also ask the audience for feedback. We would greatly appreciate suggestions for adjustments to make the app more user friendly.

##### <a href="https://goodekat.github.io/presentations/2021-isugg-shiny-fish/slides.html#1"><font color="#2B547E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2021-isugg-shiny-fish"><font color="#2B547E"> Code </font></a>

---

#### <font color="#368BC1"> Explaining Black Box Machine Learning Models </font>

**November 20, 2020** - Talk given to the ACS Data Analytics team at [Autodesk](https://www.autodesk.com/) on an overview of explainable machine learning with [Xiaodan (Annie) Lyu](https://annielyu.com/).

##### <a href="https://goodekat.github.io/presentations/2020-autodesk-eml/slides.pdf"><font color="#368BC1">Slides</font></a>

---

#### <font color="#342D7E"> The Delta Method and Applications to Mark Recapture Models</font>

**November 16, 2020** - Talk for [Bob Klaver's](https://www.nrem.iastate.edu/people/robert-klaver) ISU Ecology 607 class

This talk discusses the delta method and ways to apply it in R

##### <a href="https://goodekat.github.io/presentations/2020-isu-delta-method/slides.html#1"><font color="#342D7E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2020-isu-delta-method"><font color="#342D7E">Code</font></a>

---

#### <font color="#2B547E"> Explaining Neural Networks with Functional Data Using PCA and Feature Importance </font>

**November 14, 2020** - Talk for AAAI 2020 Fall Symposium on AI in the Government and Public Sector

Optical spectral-temporal signatures extracted from videos of explosions provide information for identifying characteristics of the corresponding explosive devices. Currently, the identification is done using heuristic algorithms and direct subject matter expert review. An improvement in predictive performance may be obtained by using machine learning, but this application lends itself to high consequence national security decisions, so it is not only important to provide high accuracy but clear explanations for the predictions to garner confidence in the model. While much work has been done to develop explainability methods for machine learning models, not much of the work focuses on situations with input variables of the form of functional data such optical spectral-temporal signatures. We propose a procedure for explaining machine learning models fit using functional data that accounts for the functional nature the data. Our approach makes use of functional principal component analysis (fPCA) and permutation feature importance (PFI). fPCA is used to transform the functions to create uncorrelated functional principal components (fPCs). The model is trained using the fPCs as inputs, and PFI is applied to identify the fPCs important to the model for prediction. Visualizations are used to interpret the variability explained by the fPCs that are found to be important by PFI to determine the aspects of the functions that are important for prediction. We demonstrate the technique by explaining neural networks fit to explosion optical spectral-temporal signatures for predicting characteristics of the explosive devices.

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. SAND2020-12004 C

##### <a href="https://goodekat.github.io/presentations/2020-aaai-fpca-pfi/slides.pdf"><font color="#2B547E">Slides</font></a>
##### <a href="https://www.youtube.com/watch?v=8UxKbfUyc2M&t=273s"><font color="#2B547E"> YouTube Video </font></a>
##### <a href="https://arxiv.org/abs/2010.12063"><font color="#2B547E">Conference Paper</font></a>

---

#### <font color="#368BC1"> A Statistical Consultant’s Perspective on Getting Help with R </font>

**October 2, 2020** - Talk given with AES Statistical Consultants (Haoyan Hu, Miranda Tilton, and Yudi Zhang) at ISU LunchinatoRs

Everyone runs into problems when writing code in R (even statisticians). Sometimes the problems are small picture such as bugs in the code. Other times the problems are large picture such as how to implement an analysis all together. In this talk, we will provide a statistical consultant’s perspective on getting help with R. We’ll tell about the resources our consulting group provides to ISU graduate students who need help with R code and/or statistical analysis for a research project. We’ll also cover resources for helping yourself, internet resources for asking questions about R, and tips for practicing a good work flow in R that will hopefully help to avoid problems.

##### <a href="https://goodekat.github.io/consulting-at-lunchinatoRs/slides.html"><font color="#368BC1">Slides</font></a>
##### <a href="https://github.com/goodekat/consulting-at-lunchinatoRs"><font color="#368BC1">Code</font></a>

---

#### <font color="#342D7E"> Explaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Feature Importance </font>

**September 24, 2020** - Talk for ISU graphics group

Explainable machine learning has become a quickly growing area of research as the use of black-box models continues to increase. While many methods have been proposed, little research has been done relating to applications involving functional data. As an intern at Sandia National Laboratories, I have been helping to develop methods to provide explanations for an application focused on predicting explosive device characteristics using optical spectral-temporal signatures from explosions. In this talk, I’ll discuss our approach that involves transforming the functions using functional principal component analysis, training neural networks on the functional principal components, and using permutation feature importance (PFI) to identify the principal components that are important for prediction. Visualization has played a key role in the interpretation of the functional principal components identified as important by PFI to understand the functional variability in the signatures that is driving the predictions made by the neural networks.

Sandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. SAND no: SAND2020-10057 A

##### <a href="https://goodekat.github.io/presentations/2020-isugg-fpca-pfi/slides_with_SAND.pdf"><font color="#342D7E">Slides</font></a>

---

#### <font color="#2B547E"> An Overview of Visualization Techniques for Explainable Machine Learning </font>

**April 10, 2020** - Talk for ISU graphics group

Machine learning models are excellent predictors, but it is impractical to interpret many of these models. Despite this impracticality, it is important to be able to explain predictions to assess and validate models. As a result, a field of research has recently developed in the explainability of machine learning models. In this talk, I will provide an overview of explainable machine learning with a focus on visualization methods. I will discuss philosophies of “explainability”, model agnostic and model specific visualization methods, and code for creating some of the visualizations in R. I hope that this talk will provide listeners with an introduction to explainable machine learning and resources to learn more if desired.

##### <a href="https://goodekat.github.io/presentations/2020-isugg-iml/slides.html#1"><font color="#2B547E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2020-isugg-iml"><font color="#2B547E">Code</font></a>

---

#### <font color="#368BC1"> Visual Diagnostics of a Model Explainer: Tools for the Assessment of LIME Explanations </font>

**December 3, 2019** - Talk on my research at Sandia

A desire to be able to interpret machine learning models has led to an area of research focused on providing explanations for predictions made by black-box models. One method that has resulted from this area of research is LIME (Ribeiro et. al. 2016). LIME stands for local interpretable model-agnostic explanations. The method provides an explanation for a black box prediction by using an interpretable model, referred to as an explainer model, to approximate the complex black-box model in a local region around a prediction of interest. The quality of the explanation produced by LIME will depend on how good of an approximation the interpretable model is to the complex model. Currently, few tools have been provided to assess this approximation. We are developing visualizations that diagnose the explanation produced by LIME. In this talk, I will provide a brief overview of LIME, motivate the importance of the assessing LIME explanations, and introduce our diagnostic visualizations.

##### <a href="https://goodekat.github.io/presentations/2019-sandia-lime-diagnostics/slides.html"><font color="#368BC1">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-sandia-lime-diagnostics"><font color="#368BC1">Code</font></a>

---

#### <font color="#342D7E"> gganimate (with a spooky twist) </font>

**October 31, 2019** - Talk for ISU graphics group

gganimate allows for the animation of ggplot2 graphics. The package has been around for a while, but it has been updated to allow for easier transitions from static ggplot2 graphic to animated versions. This talk is meant to be an interactive tutorial on how to use the updated version of gganimate. You are encouraged to bring a laptop to follow along. Since the talk will be given on Halloween, spooky datasets will be used to demonstrate the functionality of gganimate.

##### <a href="https://goodekat.github.io/presentations/2019-isugg-gganimate-spooky/slides.html"><font color="#342D7E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-isugg-gganimate-spooky"><font color="#342D7E">Code</font></a>

---

#### <font color="#2B547E"> ggResidpanel: Easy Creation of Panels of Diagnostics Plots </font>

**October 4, 2019** - Presentation for ISU lunchinatoRs

Overview and tutorial on ggResidpanel.

##### <a href="https://goodekat.github.io/presentations/2019-lunchinatoRs-ggResidpanel/slides.html"><font color="#2B547E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-lunchinatoRs-ggResidpanel"><font color="#2B547E">Code</font></a>

---

#### <font color="#368BC1"> Visual Diagnostics of a Model Explainer: Tools for the Assessment of LIME Explanations from Random Forests </font>

**July 29, 2019** - Speed presentation and e-poster for JSM 2019

Random forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. A technique called LIME was developed to provide local interpretations for predictive models. The technique fits a ridge regression model to binary encoded perturbations created from the observed data weighted by proximity to the  prediction of interest. The predicted values associated with the perturbations computed using the complex model are used as the response variable in the regression. The coefficients from the linear model are then used to determine the influential variables. We have developed some visualizations and other diagnostic tools to assess the explanations produced by LIME from a random forest. In particular, we consider how well the simple model fit by LIME captures the random forest model and how the results from LIME vary based on different algorithm input options. To demonstrate these tools, we apply LIME to a random forest fit to a forensics bullet matching dataset using the lime R package and apply our methods to diagnose the LIME explanations.

##### <a href="https://goodekat.github.io/presentations/2019-jsm-lime_diagnostics/slides.pdf"><font color="#368BC1">Slides</font></a>
##### <a href="https://goodekat.github.io/presentations/2019-jsm-lime_diagnostics/poster.pdf"><font color="#368BC1">Poster</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-jsm-lime_diagnostics"><font color="#368BC1">Code</font></a>

---

#### <font color="#342D7E"> A Review and Discussion of Residuals for Mixed Models </font>

**June 20, 2019** - Talk for NCCC-170 meeting 2019

Residuals are a key tool used to diagnose models. As a statistical consultant for researchers in many areas, I often find myself reminding my clients to visualize residuals to assess model assumptions. Many of my clients are working with mixed models, and I recently realized that I often recommend the use of certain residual types without a full understanding of the implications of selecting one type over another. This led me to have an interest in better understanding the many residuals types for mixed model. In this talk, I will provide a review of the residual types available for linear mixed models (marginal, conditional, studentized, etc.). I will explain how the residuals are computed and how these computations differ between R and SAS. I will also discuss what I have learned from the literature about how to select a residual type when assessing a model. Lastly, I will briefly touch on residual types for generalized linear mixed models and list some unanswered questions. If time permits, I will pose these remaining questions to the attendees to discuss as a group.

##### <a href="https://goodekat.github.io/presentations/2019-nccc-lmm_residuals/slides.html"><font color="#342D7E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-nccc-lmm_residuals"><font color="#342D7E">Code</font></a>

---

#### <font color="#2B547E"> Using LIME to Interpret a Random Forest Model with an Application to Bullet Matching Data </font>

**May 13, 2019** - Poster for Midwest Statistical Machine Learning Colloquium

**April 10, 2019** - Poster for Iowa State University Graduate and Professional Student Research Conference

Random forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. Recently, a technique called LIME was developed to provide local interpretations for complex predictive models. LIME determines which variables are important in a prediction of interest by fitting a local linear regression to model predictions and perturbations of the data. The coefficients from the linear model are used to interpret the complex model. While applying LIME to random forests, I encountered some unusual results. This led me to develop some diagnostic tools to evaluate LIME. I will demonstrate these by assessing the application of LIME to a random forest fit to a forensics bullet matching dataset.

##### <a href="https://goodekat.github.io/presentations/2019-gpsrc-limetorf/poster.pdf"><font color="#2B547E">Poster</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-gpsrc-limetorf"><font color="#2B547E">Code</font></a>

---

#### <font color="#368BC1"> An Application of LIME to a Random Forest Model </font>

**March 1, 2019** - Talk for ISU graphics group

Random forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. A technique called LIME was developed to provide local interpretations for black-box predictive models. In this talk, I will explain the LIME procedure and show an application of LIME to predictions from a random forest model fit to a bullet matching dataset. I will present a Shiny app I created to view the LIME explanations. Additionally, I will discuss the issues that I have encountered while working with LIME, some of the attempts at a solution, and future directions for my research.

##### <a href="https://goodekat.github.io/presentations/2019-isugg-limetorf/slides.html"><font color="#368BC1">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2019-isugg-limetorf"><font color="#368BC1">Code</font></a>

---

#### <font color="#342D7E"> Introducing ggResidpanel: An R Package for Easy Visualization of Residuals </font>

**October 12, 2018** - Talk for ISU graphics group with Dr. Katie Rey

**May 8, 2018** - Poster for Kansas State University Conference on Applied Statistics in Agriculture

As consultants on a wide variety of projects across many majors, a common oversight we encounter is a failure to examine the residuals. This is particularly true when the client is performing the analysis in R. We were inspired by the residual panel in SAS to create an R package that easily provides users with a similar panel of plots. The ggResidpanel package in R is intended to give a single view of diagnostic plots for checking the key underlying assumptions of linear models. A variety of options gives the user the ability to choose from a selection of plots to display in a window. This includes SAS’s default residual panel as well as R’s default plots for linear models. Other options have been included to ensure that this package can also be applied to deviance or Pearson residuals if the user inputs a generalized linear model. Cook’s D plots and interactive plots using Plotly are included to provide a straightforward process to identify outliers and influential points while connecting findings back to the data.

##### <a href="https://goodekat.github.io/presentations/2018-isugg-ggResidpanel/talk.html"><font color="#342D7E">Talk</font></a>
##### <a href="https://goodekat.github.io/presentations/2018-kstate-ggresidpanel/poster.pdf"><font color="#342D7E">Poster</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2018-isugg-ggResidpanel"><font color="#342D7E">Code</font></a>

---

#### <font color="#2B547E"> Interpreting Predictions from Black Box Models with LIME </font>

**November 9, 2017** - Talk for ISU graphics group

LIME (Local Interpretable Model-agnostic Explanations) is a method that explains how black box models determine individual predictions. It was developed by computer scientists at the University of Washington and implemented in Python, and recently, Thomas Pedersen developed the R package lime that allows R users to implement the procedure. In this talk, I will explain the motivation for LIME, discuss how it works, and show examples using LIME in R. Additionally, I will encourage participation as we work through an example the creators of LIME used to test the usefulness of LIME, so please bring your laptop if possible.

##### <a href="https://goodekat.github.io/presentations/2017-isugg-lime/slides.html"><font color="#2B547E">Slides</font></a>
##### <a href="https://github.com/goodekat/presentations/tree/master/2017-isugg-lime"><font color="#2B547E">Code</font></a>



