[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "My full CV is available here."
  },
  {
    "objectID": "papers.html#section",
    "href": "papers.html#section",
    "title": "Papers",
    "section": "2022",
    "text": "2022\nGoode, K., M.J. Weber, and P.M. Dixon. “WhoseEgg: Classification software for invasive carp eggs”. Submitted to PeerJ Life and Environment.\nGoode, K., Weber, M.J., Matthews, A. and Pierce, C.L. (2022), “Evaluation of a Random Forest Model to Identify Invasive Carp Eggs Based on Morphometric Features”. North Am J Fish Manage. https://doi.org/10.1002/nafm.10616\nM. A. Ausdemore, A. McCombs, D. Ries, A. Zhang, K. Shuler, K. Goode, J. D. Tucker, and J. G. Huerta. 2022. A Probabilistic Inverse “Prediction Method for Predicting Plutonium Processing Conditions. Frontiers in Nuclear Engineering-Nuclear Materials”. https://doi.org/10.3389/fnuen.2022.1083164"
  },
  {
    "objectID": "papers.html#section-1",
    "href": "papers.html#section-1",
    "title": "Papers",
    "section": "2021",
    "text": "2021\nGoode, K, Hofmann, H. “Visual diagnostics of an explainer model: Tools for the assessment of LIME explanations.” Stat Anal Data Min: The ASA Data Sci Journal. 2021; 14: 185– 200. https://doi.org/10.1002/sam.11500. GitHub repository."
  },
  {
    "objectID": "papers.html#section-2",
    "href": "papers.html#section-2",
    "title": "Papers",
    "section": "2020",
    "text": "2020\nGoode, K., Ries, D., and Zollweg, J. “Explaining Neural Networks with Functional Data Using PCA and Feature Importance”. AAAI 2020 Fall Symposium on AI in the Government and Public Sector. November 13-14, 2020. https://arxiv.org/abs/2010.12063. YouTube Video.\nDixon, P.M., Goode, K.J., and Lay, C., 2020, “Profile likelihood confidence intervals for ECx”. Iowa State Digital Library. Statistics Technical Reports. https://lib.dr.iastate.edu/stat_las_reports/1"
  },
  {
    "objectID": "papers.html#section-3",
    "href": "papers.html#section-3",
    "title": "Papers",
    "section": "2019",
    "text": "2019\nBall, E.E., Goode, K.J., and Weber, M.J., 2019. “Effects of Transport Duration and Water Quality on Age-0 Walleye Stress and Survival”, North American Journal of Aquaculture, 82:33–42. https://doi.org/10.1002/naaq.10114."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Katherine Goode",
    "section": "",
    "text": "I am a statistician with research interests in explainable machine learning and data visualization. I have a PhD in statistics from Iowa State University, an M.S. in statistics from the University of Wisconsin, Madison, and a B.A. in mathematics from Lawrence University. My PhD adviser was Heike Hofmann, and my dissertation focused on visualization techniques for explainability of machine learning (Visual diagnostics for explaining machine learning models). I currently work for Sandia National Laboratories as a research and development statistician.\n\n\n\nEmail: kjgoode@sandia.gov\nPhone: 505-844-1998\nMailing Address:\n\nStatistical Sciences\nSandia National Laboratories\nPO Box 5800 MS 0829\nAlbuquerque, NM 87185"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Below are descriptions of R packages that I’ve developed.\n\nggResidpanel\nlimeaid\nredres\nTreeTracer\n\n\n\n ggResidpanel \n\nggResidpanel is an R package Katie Rey and I developed for creating panels of diagnostic plots for residuals from a model using ggplot2. It also allows for the creation of interactive versions of the plots using plotly. The package ic available on CRAN. An overview of the package can be found at this website. The source code can be found in the GitHub repository.\nHere is an example panel of diagnostic plots created using ggResidpanel.\n\n# Fit a linear model\nlm_model1 <- lm(Volume ~ Girth, data = trees)\n\n# Use the resid_panel function from ggResidpanel to create a panel of diagnostics plots\nggResidpanel::resid_panel(lm_model1)\n\n\n\n\n\n\n\n\n\n\n\n limeaid \n\nlimeaid is an R package for assessing explanations created using the R package lime. The current implementation was developed to be used with classification models with a binary response and continuous features. The package can be installed from GitHub, and additional information on limeaid is available on the GitHub repository.\nExample of a feature heatmap depicting LIME explanation across various tuning parameters for a random forest fit to the iris data.\n\nlibrary(limeaid)\nlibrary(randomForest)\n\n# Iris training and testing\niris_test <- iris[1:5, 1:4]\niris_train <- iris[-(1:5), 1:4]\niris_lab <- iris[[5]][-(1:5)]\n\n# Fit a random forest model to the iris training data\nset.seed(20200334)\nrf <- randomForest(Species ~ .,\n                   data = cbind(iris_train, \n                                Species = iris_lab))\n\n# Run apply_lime on the iris data\nlime_applied <- apply_lime(\n  train = iris_train,\n  test = iris_test,\n  model = rf,\n  label = \"virginica\",\n  n_features = 2,\n  sim_method = c('quantile_bins',\n                 'equal_bins',\n                 'kernel_density'),\n  nbins = 2:4,\n  gower_pow = c(1, 5),\n  return_perms = TRUE,\n  seed = 20200334\n)\n\n# Extract the explanations from the apply_lime output\nexplanations <- lime_applied$explain\n\n# Create a heatmap of the features chosen\nplot_feature_heatmap(explanations)\n\n\n\n\n\n\n\n\n\n\n\n redres \n\nredres is an R package developed to help with diagnosing linear mixed models fit using the function lmer from the lme4 package. It is meant to supplement the lme4 package. redres was created by me, Kellie McClernon, Jing Zhao, Yudi Zhang, and Yonghui Huo as a project for STAT 585 (taught by Dr. Heike Hoffman). The package is currently only available on GitHub, but we hope to update it and add it to CRAN. More information can be found on the package website and GitHub repository.\nHere is an example using redres to plot the raw conditional residuals from an lmer model.\n\nlibrary(redres)\nlibrary(lme4)\n\n# Fits a linear mixed effects model\nm <- lmer(height ~ rep + treatment*variety + (1|rep:treatment) + (1|rep:treatment:variety), \n          data = paprika)\n\n# Use redres to create a residual plot using the conditional residuals\nplot_redres(m)\n\n\n\n\n\n\n\n\n\n\n\n TreeTracer \nTreeTracer is an R package for creating trace plots of trees from random forests fit using the randomForest R package. Trace plots are useful tools for visually comparing trees from a random forest. See Urbanek (2008) for additional information about trace plots. The source code can be found in the GitHub repository.\nHere is an example panel of diagnostic plots created using ggResidpanel.\n\n# Load the Palmer penguins data\npenguins <- na.omit(palmerpenguins::penguins)\n\n# Select the features for training the model\nlibrary(dplyr)\npenguin_features <- \n  penguins %>% \n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)\n\n# Fit a random forest\nset.seed(71)\npenguin_rf <-\n  randomForest::randomForest(\n    species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g,\n    data = penguins, \n    ntree = 50\n  )\n\n# Trace plots of trees in the forest\nTreeTracer::trace_plot(\n  rf = penguin_rf,\n  train = penguin_features,\n  tree_ids = 1:penguin_rf$ntree,\n  alpha = 0.4\n)"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "July 11, 2022 - Talk at Sandia National Labs\nSides GitLab"
  },
  {
    "objectID": "presentations.html#section-1",
    "href": "presentations.html#section-1",
    "title": "Presentations",
    "section": "2021",
    "text": "2021\n\nWhoseEgg: A Shiny App for Identifying Invasive Carp Using Random Forests and Fish Egg Characteristics\nApril 16, 2021 - Talk given to the ISU LunchinatoRs on the WhoseEgg app\nSlides GitLab\n\n\n\n\n\nTracing Trees: Visualizing Variability in the Architecture of Random Forest Trees Using Extensions of Trace Plots\nApril 1, 2021 - Talk for ISU Graphics Group\nRandom forests are a popular method for statistical applications with an objective of prediction. While an individual tree within a random forest provides a clear decision path for how a prediction is made, the ensemble of trees from the forest creates a complex decision process that is difficult to interpret. One approach used to gain insight into this decision process is visualization of the model. Various approaches have been taken to visualize random forests including trace plots developed by Simon Urbanek (https://link.springer.com/chapter/10.1007/978-3-540-33037-0_11). Trace plots depict the trees in a random forest using a structure similar to parallel coordinate plots that allows for a direct comparison of the trees. In this talk, I’ll describe trace plots and discuss my recent work on implementing and extending trace plots in R. I’ll also discuss my attempts to use trace plots to compare variability between clusters of trees in a random forest and visualizing representative or summary trees in the context of tree variability.\nSlides GitHub\n\n\nWhoseEgg: A Shiny App for Identifying Invasive Carp Using Random Forests and Fish Egg Characteristics\nFebruary 25, 2021 - Talk for ISU Graphics Group\nThe fish species of Grass Carp (Ctenopharyngodon idella), Silver Carp (Hypophthalmichthys molitrix), and Bighead Carp (H. nobilis) are categorized as invasive carp in North America. There is interest from a natural resource management perspective to monitor the populations and spread of the fish species. A common monitoring practice is to collect and genetically identify fish eggs, but this process is both costly in money and time. Camacho et al. (2019) demonstrated the use of machine learning as a possibility for a more efficient method of identifying invasive carp. Camacho et al. (2019) trained random forests on easy to measure egg characteristics such as water conductivity and average membrane diameter, and the models returned high accuracy.\nIn this talk, I will present my recent work with Dr. Michael Weber (NREM) and Dr. Philip Dixon on a Shiny app (WhoseEgg) for identifying invasive carp using random forests based on those from Camacho et al. (2019). The app is intended to be a tool for researchers to input their own fish egg data and easily obtain random forest predictions via a point-and-click user interface. We began work on the app in January, so it is still in development. I will share the current state of the app and our goals for enhancement. I will also ask the audience for feedback. We would greatly appreciate suggestions for adjustments to make the app more user friendly.\nSlides GitHub"
  },
  {
    "objectID": "presentations.html#section-2",
    "href": "presentations.html#section-2",
    "title": "Presentations",
    "section": "2020",
    "text": "2020\n\nExplaining Black Box Machine Learning Models\nNovember 20, 2020 - Talk given to the ACS Data Analytics team at Autodesk on an overview of explainable machine learning with Xiaodan (Annie) Lyu.\nSlides\n\n\nThe Delta Method and Applications to Mark Recapture Models\nNovember 16, 2020 - Talk for Bob Klaver’s ISU Ecology 607 class\nThis talk discusses the delta method and ways to apply it in R.\nSlides GitHub\n\n\nExplaining Neural Networks with Functional Data Using PCA and Feature Importance\nNovember 14, 2020 - Talk for AAAI 2020 Fall Symposium on AI in the Government and Public Sector\nOptical spectral-temporal signatures extracted from videos of explosions provide information for identifying characteristics of the corresponding explosive devices. Currently, the identification is done using heuristic algorithms and direct subject matter expert review. An improvement in predictive performance may be obtained by using machine learning, but this application lends itself to high consequence national security decisions, so it is not only important to provide high accuracy but clear explanations for the predictions to garner confidence in the model. While much work has been done to develop explainability methods for machine learning models, not much of the work focuses on situations with input variables of the form of functional data such optical spectral-temporal signatures. We propose a procedure for explaining machine learning models fit using functional data that accounts for the functional nature the data. Our approach makes use of functional principal component analysis (fPCA) and permutation feature importance (PFI). fPCA is used to transform the functions to create uncorrelated functional principal components (fPCs). The model is trained using the fPCs as inputs, and PFI is applied to identify the fPCs important to the model for prediction. Visualizations are used to interpret the variability explained by the fPCs that are found to be important by PFI to determine the aspects of the functions that are important for prediction. We demonstrate the technique by explaining neural networks fit to explosion optical spectral-temporal signatures for predicting characteristics of the explosive devices.\nSandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. This paper describes objective technical results and analysis. Any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the U.S. Department of Energy or the United States Government. SAND2020-12004 C\nSlides Recording Paper\n\n\nA Statistical Consultant’s Perspective on Getting Help with R\nOctober 2, 2020 - Talk given with AES Statistical Consultants (Haoyan Hu, Miranda Tilton, and Yudi Zhang) at ISU LunchinatoRs\nEveryone runs into problems when writing code in R (even statisticians). Sometimes the problems are small picture such as bugs in the code. Other times the problems are large picture such as how to implement an analysis all together. In this talk, we will provide a statistical consultant’s perspective on getting help with R. We’ll tell about the resources our consulting group provides to ISU graduate students who need help with R code and/or statistical analysis for a research project. We’ll also cover resources for helping yourself, internet resources for asking questions about R, and tips for practicing a good work flow in R that will hopefully help to avoid problems.\nSlides GitHub\n\n\nExplaining Neural Network Predictions for Functional Data Using Principal Component Analysis and Feature Importance\nSeptember 24, 2020 - Talk for ISU graphics group\nExplainable machine learning has become a quickly growing area of research as the use of black-box models continues to increase. While many methods have been proposed, little research has been done relating to applications involving functional data. As an intern at Sandia National Laboratories, I have been helping to develop methods to provide explanations for an application focused on predicting explosive device characteristics using optical spectral-temporal signatures from explosions. In this talk, I’ll discuss our approach that involves transforming the functions using functional principal component analysis, training neural networks on the functional principal components, and using permutation feature importance (PFI) to identify the principal components that are important for prediction. Visualization has played a key role in the interpretation of the functional principal components identified as important by PFI to understand the functional variability in the signatures that is driving the predictions made by the neural networks.\nSandia National Laboratories is a multimission laboratory managed and operated by National Technology & Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of Honeywell International Inc., for the U.S. Department of Energy’s National Nuclear Security Administration under contract DE-NA0003525. SAND no: SAND2020-10057 A\nSlides\n\n\nAn Overview of Visualization Techniques for Explainable Machine Learning\nApril 10, 2020 - Talk for ISU graphics group\nMachine learning models are excellent predictors, but it is impractical to interpret many of these models. Despite this impracticality, it is important to be able to explain predictions to assess and validate models. As a result, a field of research has recently developed in the explainability of machine learning models. In this talk, I will provide an overview of explainable machine learning with a focus on visualization methods. I will discuss philosophies of “explainability”, model agnostic and model specific visualization methods, and code for creating some of the visualizations in R. I hope that this talk will provide listeners with an introduction to explainable machine learning and resources to learn more if desired.\nSlides GitHub"
  },
  {
    "objectID": "presentations.html#section-3",
    "href": "presentations.html#section-3",
    "title": "Presentations",
    "section": "2019",
    "text": "2019\n\nVisual Diagnostics of a Model Explainer: Tools for the Assessment of LIME Explanations\nDecember 3, 2019 - Talk given to the statistics department at Sandia National Laboratories\nA desire to be able to interpret machine learning models has led to an area of research focused on providing explanations for predictions made by black-box models. One method that has resulted from this area of research is LIME (Ribeiro et. al. 2016). LIME stands for local interpretable model-agnostic explanations. The method provides an explanation for a black box prediction by using an interpretable model, referred to as an explainer model, to approximate the complex black-box model in a local region around a prediction of interest. The quality of the explanation produced by LIME will depend on how good of an approximation the interpretable model is to the complex model. Currently, few tools have been provided to assess this approximation. We are developing visualizations that diagnose the explanation produced by LIME. In this talk, I will provide a brief overview of LIME, motivate the importance of the assessing LIME explanations, and introduce our diagnostic visualizations.\nSlides GitHub\n\n\ngganimate (with a spooky twist)\nOctober 31, 2019 - Talk for ISU graphics group\ngganimate allows for the animation of ggplot2 graphics. The package has been around for a while, but it has been updated to allow for easier transitions from static ggplot2 graphic to animated versions. This talk is meant to be an interactive tutorial on how to use the updated version of gganimate. You are encouraged to bring a laptop to follow along. Since the talk will be given on Halloween, spooky datasets will be used to demonstrate the functionality of gganimate.\nSlides GitHub\n\n\nggResidpanel: Easy Creation of Panels of Diagnostics Plots\nOctober 4, 2019 - Presentation for ISU lunchinatoRs\nOverview and tutorial on ggResidpanel.\nSlides GitHub\n\n\nVisual Diagnostics of a Model Explainer: Tools for the Assessment of LIME Explanations from Random Forests\nJuly 29, 2019 - Speed presentation and e-poster for JSM 2019\nRandom forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. A technique called LIME was developed to provide local interpretations for predictive models. The technique fits a ridge regression model to binary encoded perturbations created from the observed data weighted by proximity to the prediction of interest. The predicted values associated with the perturbations computed using the complex model are used as the response variable in the regression. The coefficients from the linear model are then used to determine the influential variables. We have developed some visualizations and other diagnostic tools to assess the explanations produced by LIME from a random forest. In particular, we consider how well the simple model fit by LIME captures the random forest model and how the results from LIME vary based on different algorithm input options. To demonstrate these tools, we apply LIME to a random forest fit to a forensics bullet matching dataset using the lime R package and apply our methods to diagnose the LIME explanations.\nSlides Poster GitHub\n\n\nA Review and Discussion of Residuals for Mixed Models\nJune 20, 2019 - Talk for NCCC-170 meeting 2019\nResiduals are a key tool used to diagnose models. As a statistical consultant for researchers in many areas, I often find myself reminding my clients to visualize residuals to assess model assumptions. Many of my clients are working with mixed models, and I recently realized that I often recommend the use of certain residual types without a full understanding of the implications of selecting one type over another. This led me to have an interest in better understanding the many residuals types for mixed model. In this talk, I will provide a review of the residual types available for linear mixed models (marginal, conditional, studentized, etc.). I will explain how the residuals are computed and how these computations differ between R and SAS. I will also discuss what I have learned from the literature about how to select a residual type when assessing a model. Lastly, I will briefly touch on residual types for generalized linear mixed models and list some unanswered questions. If time permits, I will pose these remaining questions to the attendees to discuss as a group.\nSlides GitHub\n\n\nUsing LIME to Interpret a Random Forest Model with an Application to Bullet Matching Data\nMay 13, 2019 - Poster for Midwest Statistical Machine Learning Colloquium\nApril 10, 2019 - Poster for Iowa State University Graduate and Professional Student Research Conference\nRandom forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. Recently, a technique called LIME was developed to provide local interpretations for complex predictive models. LIME determines which variables are important in a prediction of interest by fitting a local linear regression to model predictions and perturbations of the data. The coefficients from the linear model are used to interpret the complex model. While applying LIME to random forests, I encountered some unusual results. This led me to develop some diagnostic tools to evaluate LIME. I will demonstrate these by assessing the application of LIME to a random forest fit to a forensics bullet matching dataset.\nPoster GitHub\n\n\nAn Application of LIME to a Random Forest Model\nMarch 1, 2019 - Talk for ISU graphics group\nRandom forests are known for their accurate predictive abilities, but they are a part of the family of machine learning models that lack interpretability. A technique called LIME was developed to provide local interpretations for black-box predictive models. In this talk, I will explain the LIME procedure and show an application of LIME to predictions from a random forest model fit to a bullet matching dataset. I will present a Shiny app I created to view the LIME explanations. Additionally, I will discuss the issues that I have encountered while working with LIME, some of the attempts at a solution, and future directions for my research.\nSlides GitHub"
  },
  {
    "objectID": "presentations.html#section-4",
    "href": "presentations.html#section-4",
    "title": "Presentations",
    "section": "2018",
    "text": "2018\n\nIntroducing ggResidpanel\nAn R Package for Easy Visualization of Residuals\nOctober 12, 2018 - Talk for ISU graphics group with Dr. Katie Rey\nMay 8, 2018 - Poster for Kansas State University Conference on Applied Statistics in Agriculture\nAs consultants on a wide variety of projects across many majors, a common oversight we encounter is a failure to examine the residuals. This is particularly true when the client is performing the analysis in R. We were inspired by the residual panel in SAS to create an R package that easily provides users with a similar panel of plots. The ggResidpanel package in R is intended to give a single view of diagnostic plots for checking the key underlying assumptions of linear models. A variety of options gives the user the ability to choose from a selection of plots to display in a window. This includes SAS’s default residual panel as well as R’s default plots for linear models. Other options have been included to ensure that this package can also be applied to deviance or Pearson residuals if the user inputs a generalized linear model. Cook’s D plots and interactive plots using Plotly are included to provide a straightforward process to identify outliers and influential points while connecting findings back to the data.\nSlides Poster GitHub"
  },
  {
    "objectID": "presentations.html#section-5",
    "href": "presentations.html#section-5",
    "title": "Presentations",
    "section": "2017",
    "text": "2017\n\nInterpreting Predictions from Black Box Models with LIME\nNovember 9, 2017 - Talk for ISU graphics group\nLIME (Local Interpretable Model-agnostic Explanations) is a method that explains how black box models determine individual predictions. It was developed by computer scientists at the University of Washington and implemented in Python, and recently, Thomas Pedersen developed the R package lime that allows R users to implement the procedure. In this talk, I will explain the motivation for LIME, discuss how it works, and show examples using LIME in R. Additionally, I will encourage participation as we work through an example the creators of LIME used to test the usefulness of LIME, so please bring your laptop if possible.\nSlides GitHub"
  }
]