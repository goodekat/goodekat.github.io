---
title: "Visual Diagnostics of a Model Explainer: Tools for the Assessment of LIME Explanations"
date: "December 3, 2019"
image: "https://goodekat.github.io/presentations/2019-sandia-lime-diagnostics/figures/lime-bad.png"
---

*Talk given to the statistics department at Sandia National Laboratories*

A desire to be able to interpret machine learning models has led to an area of research focused on providing explanations for predictions made by black-box models. One method that has resulted from this area of research is LIME (Ribeiro et. al. 2016). LIME stands for local interpretable model-agnostic explanations. The method provides an explanation for a black box prediction by using an interpretable model, referred to as an explainer model, to approximate the complex black-box model in a local region around a prediction of interest. The quality of the explanation produced by LIME will depend on how good of an approximation the interpretable model is to the complex model. Currently, few tools have been provided to assess this approximation. We are developing visualizations that diagnose the explanation produced by LIME. In this talk, I will provide a brief overview of LIME, motivate the importance of the assessing LIME explanations, and introduce our diagnostic visualizations.

[Slides](https://goodekat.github.io/presentations/2019-sandia-lime-diagnostics/slides.html) [GitHub](https://github.com/goodekat/presentations/tree/master/2019-sandia-lime-diagnostics)

<center>![](https://goodekat.github.io/presentations/2019-sandia-lime-diagnostics/figures/lime-bad.png){width="75%"}</center>
